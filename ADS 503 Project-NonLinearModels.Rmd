---
title: "ADS 503 Project v2"
author: "Brianne Bell"
date: '2022-06-14'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Libraries
## REMOVE NOTE LATER - ggplot and dplyr are part of tidyverse, so removed them. 
```{r message=FALSE, warning=FALSE}
library(haven)
library(caret)
library(gridExtra)
library(corrplot)
library(e1071)
library(car)
library(lattice)
library(doParallel)
library(RANN)
library(tidyverse)
library(earth)
```

# Set up Parallelization - Note, you can reduce number of clusters if needed not sure if R does it automatically if you set over the number of cores on your CPU
```{r}
cl <- makeCluster(4)
registerDoParallel(cl)
```


# Read in data
```{r}
Demographic <- read_xpt("P_DEMO.XPT")
BodySize <- read_xpt("P_BMX.XPT")
Chol_ldl <- read_xpt("P_TRIGLY.XPT")
```

# Create Dataset
Chol-ldl
```{r}
#Select Variables of interest
Chol_ldl <- Chol_ldl %>% select(SEQN, LBDLDL)
#NA in target feature won't be useful
Chol_ldl <- Chol_ldl %>% drop_na()
```

Demographic
```{r}
#Get rid of variables we don't need
Drop_col <- c('SDDSRVYR', 'RIDSTATR', 'RIDEXMON', 'SIAPROXY', 'SIAINTRP', 'FIAPROXY', 'FIAINTRP', 'MIAPROXY', 'MIAINTRP', 'WTINTPRP', 'WTMECPRP', 'SDMVPSU', 'SDMVSTRA')
Demographic <- Demographic %>% select(-one_of(Drop_col))
```

BodySize
```{r}
Drop_col <- c('BMIWT', 'BMIRECUM', 'BMIHEAD', 'BMIHT', 'BMILEG', 'BMIARML', 'BMIARMC', 'BMIWAIST', 'BMIHIP', 'BMDSTATS')
BodySize <- BodySize %>% select(-one_of(Drop_col))
```

Join
```{r}
J1 <- Chol_ldl %>% left_join(Demographic, by = "SEQN")
Chol <- J1 %>% left_join(BodySize, by = "SEQN")
Chol <- Chol %>% select(!SEQN)
```

# Cleaning
## Changing factors for EDA 
## Changing Variables to the Correct Type 
```{r}
Chol_2 <- Chol
factors <- c("RIAGENDR", "RIDRETH1", "RIDRETH3", "DMDBORN4", "DMDEDUC2", "DMDMARTZ", "RIDEXPRG", "SIALANG", "FIALANG", "MIALANG", "AIALANGA")
Chol_2[,factors] <- lapply(Chol_2[,factors], factor)
```

## Change factor levels to be more interpretable
```{r}
levels(Chol_2$RIAGENDR) <- c("Male", "Female")
levels(Chol_2$RIDRETH1) <- c("Mex", "OHis", "White", "Black", "Oth")
levels(Chol_2$RIDRETH3) <- c("Mex", "OHis", "White", "Black", "Asian", "Oth")
levels(Chol_2$DMDBORN4) <- c("USA", "Oth", "Ref", "DK")
levels(Chol_2$DMDYRUSZ) <- c("<5", "5-15", "15-30", ">30", "Ref", "DK")
levels(Chol_2$DMDEDUC2) <- c("<9", "9-11", "HS", "AA", "BS+", "Ref", "DK")
levels(Chol_2$DMDMARTZ) <- c("Mar", "Sep", "Nev", "Ref", "DK")
levels(Chol_2$RIDEXPRG) <- c("Yes", "No", "DK")
levels(Chol_2$SIALANG) <- c("English", "Spanish")
levels(Chol_2$FIALANG) <- c("English", "Spanish")
levels(Chol_2$MIALANG) <- c("English", "Spanish")
levels(Chol_2$AIALANGA) <- c("English", "Spanish", "Asian")
```


# EDA
## NAs
### By Variable - Removed variables with over 3000 observations missing
```{r message=FALSE, warning=FALSE}
Variable_na <- Chol_2 %>% select(everything()) %>% summarise_all(funs(sum(is.na(.)))) %>% pivot_longer(cols = c(colnames(Chol_2[,1:ncol(Chol_2)])), names_to = "Variable", values_to = "Missing") %>% arrange(desc(Missing))
Drop_col <- c("RIDAGEMN", "BMXRECUM", "BMXHEAD", "BMDBMIC", "RIDEXPRG", "DMDYRUSZ")
Chol_2 <- Chol_2 %>% select(-one_of(Drop_col))
```

### By Row
```{r}
row_na <- rowSums(is.na(Chol_2))
row_na <- data.frame(row_na, Row = c(1:length(row_na)))
row_na <- row_na %>% arrange(desc(row_na))
#Most missing values in a row is 12, not bad
```

## Distributions
### Response - LDL Cholesterol
```{r}
#Looks like a fairly normal distribution, maybe a little skewed to the right. 
ggplot(Chol_2, aes(x = LBDLDL)) + geom_histogram() + ggtitle("LDL Distribution Histogram")
skewness(Chol_2$LBDLDL)
#skewness value .7886403 confirms very mild skewness to the right
```

### Predictors
Factors
```{r}
Chol_fact <- Chol_2 %>% select_if(is.factor)
Chol.bar <- function(xvar){
  ggplot(Chol_fact, aes_(x = as.name(xvar))) +
    geom_bar(color = "black") + coord_flip()
}
Lang_barplots <- lapply(names(Chol_fact[,7:10]), Chol.bar)
Oth_barplots <- lapply(names(Chol_fact[,1:6]), Chol.bar)
grid.arrange(grobs = Lang_barplots, top = "Language Features")
grid.arrange(grobs = Oth_barplots, top = "Other Demographics")
```

Numeric
```{r message=FALSE, warning=FALSE}
Chol_num <- Chol_2 %>% select_if(is.numeric) %>% select(!LBDLDL)
Chol.hist <- function(xvar){
  ggplot(Chol_num, aes_(x = as.name(xvar))) +
    geom_histogram(color = "black") 
}
Dem_hist <- lapply(names(Chol_num[,1:2]), Chol.hist)
Body_hist <- lapply(names(Chol_num[,3:10]), Chol.hist)
grid.arrange(grobs = Dem_hist, top = "Demographic Features")
grid.arrange(grobs = Body_hist, top = "Body Measures")
```

## Correlations
Heatmap
```{r}
Chol_dummy <- fastDummies::dummy_cols(Chol_2)
Chol_dummy <- Chol_dummy %>% select_if(~!is.factor(.))
Chol_dummy[] <- lapply(Chol_dummy, as.numeric)
Chol_cor <- cor(Chol_dummy, use = "complete.obs")
Chol_corplot <- corrplot(cor(Chol_dummy, use = "complete.obs"), tl.pos = 'n')
#Looks like some dummy variables that are refusal could be messing up correlations
Drop_col <- c("DMDBORN4_Ref", "DMDBORN4_DK", "DMDEDUC2_Ref", "DMDEDUC2_DK", "DMDEDUC2_NA", "DMDMARTZ_Ref", "DMDMARTZ_NA", "FIALANG_NA", "MIALANG_NA", "AIALANGA_NA")
Chol_dummy_2 <- Chol_dummy %>% select(-one_of(Drop_col))
cor(Chol_dummy_2, use = "complete.obs")
corrplot(cor(Chol_dummy_2, use = "complete.obs"), tl.pos = 'n', type = 'lower')
```
```{r}
# Mini Correlations: Sociological Measures:
socio <- Chol_dummy_2[,c("RIAGENDR_Male","RIAGENDR_Female",
                       "RIDRETH1_Mex", "RIDRETH1_OHis", "RIDRETH1_White", 
                       "RIDRETH1_Black", "RIDRETH1_Oth", "RIDRETH3_Mex", "RIDRETH3_OHis",
                       "RIDRETH3_White", "RIDRETH3_Black", "RIDRETH3_Asian", "RIDRETH3_Oth",
                       "DMDBORN4_USA", "DMDBORN4_Oth", "DMDEDUC2_<9", "DMDEDUC2_9-11",
                       "DMDEDUC2_HS", "DMDEDUC2_AA", "DMDEDUC2_BS+","DMDMARTZ_Mar",
                       "DMDMARTZ_Sep", "DMDMARTZ_Nev", "DMDMARTZ_DK", "SIALANG_English", 
                       "SIALANG_Spanish", "FIALANG_English", "FIALANG_Spanish",
                       "MIALANG_English","MIALANG_Spanish", "AIALANGA_English",
                       "AIALANGA_Spanish", "AIALANGA_Asian", "LBDLDL")]
cor(socio, use = "complete.obs")
corrplot(cor(socio, use = "complete.obs"), tl.pos = 'y', type = 'lower', 
         order = "hclust", tl.cex = 0.5)
```
```{r}
# Mini Correlations: Biological Measures:
biologic <- Chol_dummy_2[,c("RIDAGEYR", "BMXWT", "BMXHT", "BMXBMI", 
                            "BMXLEG", "BMXARML", "BMXARMC", "BMXWAIST",
                            "BMXHIP", "RIAGENDR_Male", "RIAGENDR_Female","LBDLDL")]
cor(biologic, use = "complete.obs")
corrplot(cor(biologic, use = "complete.obs"), tl.pos = 'y', type = 'lower', 
         order = "hclust", tl.cex = 0.8)
# correlations between certain biological measures make sense. BMI is derived from the MASS and height of an individual, so it makes sense that many of the BMI measurements correlate with each other. (i.e. hip, waist, and weight measurements correlate with a higher BMI. Being female correlates negatively with leg and arm length as well as height)
```

# REMOVE NOTE LATER - MAY NEED TO USE DUMMY CHOL DATA FOR HIGH CORRELATIONS - REGULAR CHOL STILL HAS CATEGORICAL VARIABLES AS SINGLE NUMERIC VARIABLES.
```{r}
# let's check for highly correlated predictors
# we'll do this on our non factor transformed dataset
dim(Chol)
# 27 variables. let's find correlations greater than 0.80 and see how the data looks if removed
corr_Chol <- cor(Chol)
# if removed, how many variables are left
high_corr_Chol <- findCorrelation(corr_Chol, cutoff = 0.80)
no_corr_Chol <- Chol[, -high_corr_Chol]
dim(no_corr_Chol)
```
# Checking things above with Dummy Variable Data
## CURRENTLY TESTING STUFF HERE FOR HOW TO DEAL WITH DUMMY DATA WITH COR
```{r}
testdata <- Chol_dummy
testdata <- apply(Chol_dummy, 2, function(y) {y[!is.finite(y)] = NA; y})
testdata <- testdata[complete.cases(testdata), ]
testdata <- testdata[, -nearZeroVar(testdata)]
testdata <- data.frame(testdata)
dim(Chol_dummy)
Cor_chol_dummy <- cor(testdata)
high_corr_dummy <- findCorrelation(Cor_chol_dummy, cutoff = 0.8)
```

```{r}
# looking at a base linear model, to see significant variables 
model0 <- lm(LBDLDL~., Chol_2)
summary(model0)
  # significant contributors: variable (Pr(>|t|)) 
    # RIDAGEYR (0.000132)
    # (Intercept) (0.043474)
    # MDBORN4Oth (0.076414) 
    # AIALANGASpanish (0.041803)
    # BMXLEG (0.033051)
    # BMXARMC (0.004507) 
    # BMXWAIST (0.071888)
```

```{r}
# looking at VIF for baseline linear:
vif(model0)
  # highly/perfectly correlated factors, we might need to drop some
```

# Degenerate Predictors
```{r}
# Let's check for degenerate predictors from the original dataset
nearZeroVar(Chol, saveMetrics = FALSE)
deg_chol <- subset(Chol, select=c(4,11,16,18,19))
colnames(deg_chol)
# Do it again on factor dataset
nearZeroVar(Chol_2, saveMetrics = FALSE)
deg_chol2 <- subset(Chol_2, select=c(13))
colnames(deg_chol2)
# we may have to consider removing depending on the data used for modeling
```



# Splitting
#NOTE SWITCHED TRAINING TO DUMMY
```{r}
# set the seed and split the data. We'll do an 80/20 split
set.seed(123)
Chol_split <- createDataPartition(Chol$LBDLDL, p=0.80, list=FALSE)
# split into train and test
Chol_train <- Chol_dummy[Chol_split,]
Chol_test <- Chol_dummy[-Chol_split,]
# split predictors from the target
Chol_train_X <- as.data.frame(subset(Chol_train, select=-c(LBDLDL))) 
Chol_train_y <- Chol_train$LBDLDL
Chol_test_X <- as.data.frame(subset(Chol_test, select=-c(LBDLDL)))
Chol_test_y <- Chol_test$LBDLDL
# Creating imputed data sets
Chol_imp <- preProcess(Chol_train_X, method = c("center", "scale", "knnImpute"))
Chol_train_X_imp <- predict(Chol_imp, Chol_train_X)
Chol_test_X_imp <- predict(Chol_imp, Chol_test_X)
# Adding Resampling/Validation Set and Control 
set.seed(123)
Chol_folds <- createFolds(y = Chol_train_X, k = 10, returnTrain = T)
Chol_control <- trainControl(method = "cv", index = Chol_folds)
```


# Linear Models - Hunter
## OLS 
### Create Initial Model
```{r}
# Chol_ols_tune <- train(x = Chol_train_X_imp, y = Chol_train_y, method = "lm", trControl = Chol_control)
# plot(Chol_ols_tune$finalModel)
```
### FIX TRAINING SET
```{r}
# VIF shows aliased coefficients, need to get rid of those by removing high cor predictors
test <- cor(Chol_train_X_imp)
# Also have an issue with DMDEDUC2_DK all being zero so get rid of high var predictors
Chol_tr_x_imp_vr <- Chol_train_X_imp[, -nearZeroVar(Chol_train_X_imp)]
Chol_tr_X_imp_fin <- Chol_tr_x_imp_vr[, -findCorrelation(cor(Chol_tr_x_imp_vr), cutoff = 0.9)] 
```

### Tune Another Model
```{r}
# Chol_ols_tune2 <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "lm", trControl = Chol_control)
# plot(Chol_ols_tune2$finalModel)
# summary(Chol_ols_tune2$finalModel)
```

### Final OLS Model
```{r}
# Chol_sig_tr <- Chol_tr_X_imp_fin %>% select(RIDAGEYR, BMXHT, BMXBMI, BMXLEG, BMXARMC,  DMDBORN4_Oth, DMDEDUC2_NA, MIALANG_NA, AIALANGA_NA)
# Chol_ols <- train(x = Chol_sig_tr, y = Chol_train_y, method = "lm", trControl = Chol_control)
# plot(Chol_ols$finalModel)
# summary(Chol_ols$finalModel)
# #Predict on test data
# Chol_ols_res <- predict(Chol_ols, Chol_test_X)
```
# Non-Linear Models - Brianne
## Support Vector Machine (SVM)
### Create Initial Radial Model
```{r message=FALSE, warning=FALSE}
# initial SVM model with radial basis and processed Chol_tr_X_imp_fin and Chol_train_y
set.seed(123)
svmR0 <- train(x=Chol_tr_X_imp_fin, y = Chol_train_y,
               method = "svmRadial",
               preProcess = c("center", "scale"),
               tuneLength = 14,
               trControl = Chol_control)
svmR0
  # final model uses: sigma = 0.0207376 and C = 0.25
  # RMSE: 35.40559, Rsquared: 0.019455264
plot(svmR0, scales = list(x = list(log = 2)), main="SVM-Radial Initial")
```

### final radial model
```{r}
# svm radial model v2 
    # issue causing variables in X: MIALANG_Spanish, DMDEDUC2_<9, MIALANG_NA (zero var)
Chol_tr_X_impfin_drop <- c("MIALANG_Spanish", "DMDEDUC2_<9", "MIALANG_NA")
Chol_tr_X_impfin_sv <- subset(Chol_tr_X_imp_fin, 
                              select = !(names(Chol_tr_X_imp_fin) %in% Chol_tr_X_impfin_drop))
#making test X have same columns available
Chol_te_X_sv <- subset(Chol_test_X_imp, select = c(names(Chol_tr_X_impfin_sv)))
dim(Chol_tr_X_impfin_sv)
dim(Chol_te_X_sv)
#model
set.seed(123)
svmR1 <- train(x=Chol_tr_X_impfin_sv, y = Chol_train_y,
               method = "svmRadial",
               preProcess = c("center", "scale"),
               tuneLength = 14,
               trControl = Chol_control)
svmR1
  # final model uses: sigma = 0.02231109 and C = 0.25.
  # RMSE: 35.32135, Rsquared: 0.018362258 
plot(svmR1, scales = list(x = list(log = 2)), main="SVM-Radial v2")
svmR1$finalModel
```

### Create Polynomial Model
```{r}
# going to use the x training set from final svm-radial due to assuming there will be the same problem causing factors of nearZeroVar.
set.seed(123)
svmP <- train(x=Chol_tr_X_impfin_sv, y = Chol_train_y,
               method = "svmPoly",
               preProcess = c("center", "scale"),
               tuneGrid = expand.grid(degree = 1:2, 
                                      scale = c(0.01, 0.005, 0.001), 
                                      C = 2^(-2:5)),
               trControl = Chol_control)
svmP
  # final model uses: degree = 2, scale =  0.001,  offset =  1 
  # sigma = 0.02231109 and C = 0.25.
  # RMSE: 35.57248, Rsquared: 0.004595661
plot(svmP, scales = list(x = list(log = 2),
                         between=list(x=.5, y=1)), main="SVM-Polynomial")
svmP$finalModel
```


## K Nearest Neighbors (KNN)
### Create Initial KNN Model
```{r}
# KNN Model needs to have NZV removed so again we are using the x=Chol_tr_X_impfin_sv to train and Chol_te_X_sv to test
set.seed(123)
knnTune <- train(x=Chol_tr_X_impfin_sv, y = Chol_train_y,
               method = "knn",
               preProcess = c("center", "scale"),
               tuneGrid = data.frame(k=1:40),
               trControl = Chol_control)
knnTune
  # final model uses: k=40
  # RMSE: 35.63394, Rsquared: 0.0006650456
plot(knnTune, main="KNN")
knnTune$finalModel
```


## Multivariate Adaptive Regression Splines (MARS)
### Create Initial MARS Model
```{r}
# MARS model doesn't need preprocessing, so first rendition will be with Chol_tr_X_imp_fin 
set.seed(123)
mars1 <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y,
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:3, nprune = 2:38),
                   trControl = Chol_control)
mars1$finalModel
mars1$results
  #used 1 of 32 predictors, 2 of 18 terms (nprune =2) degree=1
  # RMSE: 35.02993, Rsquared:	0.05354368	
plot(mars1, main="MARS Initial")
```


## Multivariate Adaptive Regression Splines (MARS)
### Create Secondary MARS Model
```{r}
# MARS model using same X sets as SVM models: 
set.seed(123)
mars2 <- train(x = Chol_tr_X_impfin_sv, y = Chol_train_y,
                   method = "earth",
                   tuneGrid = expand.grid(degree = 1:3, nprune = 2:38),
                   trControl = Chol_control)
mars2$finalModel
plot(mars2, main="MARS Secondary")
# No change between the two MARS models. Drops all but two factors for both.
  #nprune=2, degree=1, RMSE: 34.80857, Rsquared: 0.05457991
```


## Neural Network Model (nnet)
### Create Initial NNET Model
```{r}
set.seed(123)
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(3, 7, 11, 13))
# NNET first rendition will be with Chol_tr_X_imp_fin 
set.seed(100)
nnet1 <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y,
                  method = "nnet",
                  tuneGrid = nnetGrid,
                  trControl = Chol_control,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 13 * (ncol(Chol_tr_X_imp_fin) + 1) + 13 + 1,
                  maxit = 100)
nnet1
  # size=3, decay=0.1
  # RMSE: 40.97813, Rsquared: 0.0051328366
plot(nnet1, main="NNET first")
```
```{r}
set.seed(123)
# NNET second rendition will be with Chol_tr_X_impfin_sv 
set.seed(100)
nnet2 <- train(x = Chol_tr_X_impfin_sv, y = Chol_train_y,
                  method = "nnet",
                  tuneGrid = nnetGrid,
                  trControl = Chol_control,
                  preProc = c("center", "scale"),
                  linout = TRUE,
                  trace = FALSE,
                  MaxNWts = 13 * (ncol(Chol_tr_X_impfin_sv) + 1) + 13 + 1,
                  maxit = 100)
nnet2
  # size=13, decay=0.1
  # RMSE: 44.38061, Rsquared: 0.001743654
plot(nnet2, main="NNET second")
```
## Comparing Nonlinear Models:
### Saving Results
```{r}
NonLpred <- data.frame(obs=Chol_test_y)
NonLpred$svmR <- predict(svmR1, Chol_te_X_sv)
NonLpred$svmP <- predict(svmP, Chol_te_X_sv)
NonLpred$KNN <- predict(knnTune, Chol_te_X_sv)
NonLpred$MARS2 <- predict(mars2, Chol_te_X_sv)
NonLpred$NNET2 <- predict(nnet2, Chol_te_X_sv)
plotpred <- data.frame(x=1:921, y1=NonLpred$obs, y2=NonLpred$svmR, 
                       y3=NonLpred$svmP, y4=NonLpred$KNN, 
                       y5=NonLpred$MARS2[,"y"], y6=NonLpred$NNET2)
plot(plotpred$x, plotpred$y1, type = "l", col = 1, 
     xlab = "prediction #", ylab = "prediction values", 
     main = "Nonlinear Model Predictions")
lines(plotpred$x, plotpred$y2, col = 2)
lines(plotpred$x, plotpred$y3, col = 3)
lines(plotpred$x, plotpred$y4, col = 4)
lines(plotpred$x, plotpred$y5, col = 5)
lines(plotpred$x, plotpred$y6, col = 6)
legend("bottomleft", cex=0.5, legend = c("Observed", "SVM-Radial",
                            "SVM-Polynomial", "KNN", "MARS", "NNET"),
       col = 1:6, lwd = 2)
```
### getting RMSE and Plotting
```{r}
# RMSE = sqrt(sum((obs-pred)^2)/n), n=921
getRMSE <- function(x,y) {
 sqrt(sum((x-y)^2)/length(x))
}
nonlin_rmse <- data.frame(c("svmRad", "svmPoly", "KNN",
                            "MARS", "NNet"))
nonlin_rmse$RMSE <- c(getRMSE(NonLpred$obs, NonLpred$svmR),
                      getRMSE(NonLpred$obs, NonLpred$svmP),
                      getRMSE(NonLpred$obs, NonLpred$KNN),
                      getRMSE(NonLpred$obs, NonLpred$MARS2),
                      getRMSE(NonLpred$obs, NonLpred$NNET2))
colnames(nonlin_rmse)[1]<-"Model Type"
nonlin_rmse[order(nonlin_rmse$RMSE),]
  # best non linear model is svm radial with 
    # sigma = 0.022 and cost=0.25 (RMSE is 32.86)
```

# messing around with predictor set and log transforming the response
```{r}
expDropC <- c("RIDRETH1_Black", "RIDRETH1_Mex", "RIDRETH1_OHis", 
               "RIDRETH1_Oth", "RIDRETH1_White", "BMXBMI")
Xtrial_tr <- subset(Chol_train_X_imp, select = !(names(Chol_train_X_imp) %in% expDropC))
# looking for near zero var:
Xtri_nzv <- nearZeroVar(Xtrial_tr)
  # "DMDBORN4_Ref", "DMDBORN4_DK", "DMDEDUC2_Ref", "DMDEDUC2_DK", "DMDMARTZ_Ref",
    # "DMDMARTZ_DK", "AIALANGA_Asian"
Xtri_tr_nz <- subset(Xtrial_tr, select = -c(Xtri_nzv))
print(paste("Xtrial_tr ncol: ", ncol(Xtrial_tr), " NZV removed, new ncol: ", ncol(Xtri_tr_nz)))

# dropping SIALANG groups (sample person interview instrument lang)
expDropC <- c("SIALANG_English", "SIALANG_Spanish", 
              "MIALANG_English", "MIALANG_Spanish", "MIALANG_NA")
Xtri_tr_nz <- subset(Xtri_tr_nz, select = !(names(Xtri_tr_nz) %in% expDropC))
print(paste("Xtrial_tr ncol: ", ncol(Xtrial_tr), " NZV removed, new ncol: ", ncol(Xtri_tr_nz)))

#looking for high corr:
Xtritr_hiC <- findCorrelation(cor(Xtri_tr_nz), cutoff = 0.8)
Xtritr_hiC
  # "AIALANGA_English", "DMDBORN4_USA", "BMXWT", "FIALANG_English", 
  # "BMXWAIST", "BMXHIP", "DMDEDUC2_NA", "RIAGENDR_Male"

Xtritr_hiC <- subset(Xtri_tr_nz, select = c("AIALANGA_English", "DMDBORN4_USA", 
                                            "BMXWT", "FIALANG_English", "BMXWAIST",
                                            "BMXHIP", "DMDEDUC2_NA", "RIAGENDR_Male"))
cor(Xtritr_hiC)
corrplot(cor(Xtritr_hiC), order = "hclust", type="lower")

#dropping "BMXHIP", "BMXWAIST", DMDEDUC2_<9 (recurring issues in model attempts)
drophiC <- c("BMXHIP", "BMXWAIST", "DMDEDUC2_<9")
X_trial_train <- subset(Xtri_tr_nz, select = !(names(Xtri_tr_nz) %in% drophiC))
corrplot(cor(X_trial_train), order = "hclust", type="lower")

```

```{r}
# making X_trial_test match columns of X_trial_train
keepsies <- colnames(X_trial_train)
X_trial_test <- subset(Chol_test_X_imp, select = c(keepsies))
```


```{r}
#svm radial with X_trial_train and log adjusted y
log_Y_train <- log10(Chol_train_y)
hist(log_Y_train)
log_Y_test <- log10(Chol_test_y)

#model
set.seed(123)
svmR_trial <- train(x=X_trial_train, y = log_Y_train,
               method = "svmRadial",
               preProcess = c("center", "scale"),
               tuneLength = 14,
               trControl = Chol_control)
svmR_trial
# issues in: DMDEDUC2_<9, 
  # final model uses: sigma = 0.02019005 and C = 0.25.
  # RMSE: 0.1531765, Rsquared: 0.021637157
    # while these are the lowest values, the graph is identical to the non-log adjusted SVM radial model with just different RMSE values. 
plot(svmR_trial, scales = list(x = list(log = 2)), main="SVM-Radial with X_trial_train and Log Y")
svmR_trial$finalModel
trialPred <- data.frame(obs=log_Y_test)
trialPred$svmRad <- predict(svmR_trial, X_trial_test)
```

