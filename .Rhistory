levels(Chol_2$DMDEDUC2) <- c("<9", "9-11", "HS", "AA", "BS+", "Ref", "DK")
levels(Chol_2$DMDMARTZ) <- c("Mar", "Sep", "Nev", "Ref", "DK")
levels(Chol_2$RIDEXPRG) <- c("Yes", "No", "DK")
levels(Chol_2$SIALANG) <- c("English", "Spanish")
levels(Chol_2$FIALANG) <- c("English", "Spanish")
levels(Chol_2$MIALANG) <- c("English", "Spanish")
levels(Chol_2$AIALANGA) <- c("English", "Spanish", "Asian")
Variable_na <- Chol_2 %>% select(everything()) %>% summarise_all(funs(sum(is.na(.)))) %>% pivot_longer(cols = c(colnames(Chol_2[,1:ncol(Chol_2)])), names_to = "Variable", values_to = "Missing") %>% arrange(desc(Missing))
Drop_col <- c("RIDAGEMN", "BMXRECUM", "BMXHEAD", "BMDBMIC", "RIDEXPRG", "DMDYRUSZ")
Chol_2 <- Chol_2 %>% select(-one_of(Drop_col))
row_na <- rowSums(is.na(Chol_2))
row_na <- data.frame(row_na, Row = c(1:length(row_na)))
row_na <- row_na %>% arrange(desc(row_na))
#Most missing values in a row is 12, not bad
#Looks like a fairly normal distribution, maybe a little skewed to the right.
ggplot(Chol_2, aes(x = LBDLDL)) + geom_histogram() + ggtitle("Distribution of LDL Cholesterol") + xlab("LDL Chol.") + ylab("Count")
skewness(Chol_2$LBDLDL)
#skewness value .7886403 confirms very mild skewness to the right
Chol_fact <- Chol_2 %>% select_if(is.factor)
Chol.bar <- function(xvar){
ggplot(Chol_fact, aes_(x = as.name(xvar))) +
geom_bar(color = "black") + coord_flip()
}
Lang_barplots <- lapply(names(Chol_fact[,7:10]), Chol.bar)
Oth_barplots <- lapply(names(Chol_fact[,1:6]), Chol.bar)
grid.arrange(grobs = Lang_barplots, top = "Language Features")
grid.arrange(grobs = Oth_barplots, top = "Other Demographics")
Chol_num <- Chol_2 %>% select_if(is.numeric) %>% select(!LBDLDL)
Chol.hist <- function(xvar){
ggplot(Chol_num, aes_(x = as.name(xvar))) +
geom_histogram(color = "black")
}
Dem_hist <- lapply(names(Chol_num[,1:2]), Chol.hist)
Body_hist <- lapply(names(Chol_num[,3:10]), Chol.hist)
grid.arrange(grobs = Dem_hist, top = "Demographic Features")
grid.arrange(grobs = Body_hist, top = "Body Measures")
Chol_dummy <- fastDummies::dummy_cols(Chol_2)
Chol_dummy <- Chol_dummy %>% select_if(~!is.factor(.))
Chol_dummy[] <- lapply(Chol_dummy, as.numeric)
Chol_cor <- cor(Chol_dummy, use = "complete.obs")
Chol_corplot <- corrplot(cor(Chol_dummy, use = "complete.obs"), tl.pos = 'n')
#Looks like some dummy variables that are refusal could be messing up correlations
Drop_col <- c("DMDBORN4_Ref", "DMDBORN4_DK", "DMDEDUC2_Ref", "DMDEDUC2_DK", "DMDEDUC2_NA", "DMDMARTZ_Ref", "DMDMARTZ_NA", "FIALANG_NA", "MIALANG_NA", "AIALANGA_NA")
Chol_dummy_2 <- Chol_dummy %>% select(-one_of(Drop_col))
invisible(cor(Chol_dummy_2, use = "complete.obs")) # using invisible() to reduce extensive output
corrplot(cor(Chol_dummy_2, use = "complete.obs"), tl.pos = 'n', type = 'lower')
# Mini Correlations: Sociological Measures:
socio <- Chol_dummy_2[,c("RIAGENDR_Male","RIAGENDR_Female",
"RIDRETH1_Mex", "RIDRETH1_OHis", "RIDRETH1_White",
"RIDRETH1_Black", "RIDRETH1_Oth", "RIDRETH3_Mex", "RIDRETH3_OHis",
"RIDRETH3_White", "RIDRETH3_Black", "RIDRETH3_Asian", "RIDRETH3_Oth",
"DMDBORN4_USA", "DMDBORN4_Oth", "DMDEDUC2_<9", "DMDEDUC2_9-11",
"DMDEDUC2_HS", "DMDEDUC2_AA", "DMDEDUC2_BS+","DMDMARTZ_Mar",
"DMDMARTZ_Sep", "DMDMARTZ_Nev", "DMDMARTZ_DK", "SIALANG_English",
"SIALANG_Spanish", "FIALANG_English", "FIALANG_Spanish",
"MIALANG_English","MIALANG_Spanish", "AIALANGA_English",
"AIALANGA_Spanish", "AIALANGA_Asian", "LBDLDL")]
invisible(cor(socio, use = "complete.obs")) # using invisible() to reduce extensive output
corrplot(cor(socio, use = "complete.obs"), tl.pos = 'y', type = 'lower',
order = "hclust", tl.cex = 0.5)
# Mini Correlations: Biological Measures:
biologic <- Chol_dummy_2[,c("RIDAGEYR", "BMXWT", "BMXHT", "BMXBMI",
"BMXLEG", "BMXARML", "BMXARMC", "BMXWAIST",
"BMXHIP", "RIAGENDR_Male", "RIAGENDR_Female","LBDLDL")]
invisible(cor(biologic, use = "complete.obs")) # using invisible() to reduce extensive output
corrplot(cor(biologic, use = "complete.obs"), tl.pos = 'y', type = 'lower',
order = "hclust", tl.cex = 0.8)
# correlations between certain biological measures make sense. BMI is derived from the MASS and height of an individual, so it makes sense that many of the BMI measurements correlate with each other. (i.e. hip, waist, and weight measurements correlate with a higher BMI. Being female correlates negatively with leg and arm length as well as height)
# let's check for highly correlated predictors
# we'll do this on our non factor transformed dataset
dim(Chol)
# 27 variables. let's find correlations greater than 0.80 and see how the data looks if removed
corr_Chol <- cor(Chol)
# if removed, how many variables are left
high_corr_Chol <- findCorrelation(corr_Chol, cutoff = 0.80)
no_corr_Chol <- Chol[, -high_corr_Chol]
dim(no_corr_Chol)
# looking at a base linear model, to see significant variables
model0 <- lm(LBDLDL~., Chol_2)
summary(model0)
# significant contributors: variable (Pr(>|t|))
# RIDAGEYR (0.000132)
# (Intercept) (0.043474)
# MDBORN4Oth (0.076414)
# AIALANGASpanish (0.041803)
# BMXLEG (0.033051)
# BMXARMC (0.004507)
# BMXWAIST (0.071888)
# looking at VIF for baseline linear:
#vif(model0)
# highly/perfectly correlated factors, we might need to drop some
# Let's check for degenerate predictors from the original dataset
nearZeroVar(Chol, saveMetrics = FALSE)
deg_chol <- subset(Chol, select=c(4,11,16,18,19))
colnames(deg_chol)
# Do it again on factor dataset
nearZeroVar(Chol_2, saveMetrics = FALSE)
deg_chol2 <- subset(Chol_2, select=c(13))
colnames(deg_chol2)
# we may have to consider removing depending on the data used for modeling
# set the seed and split the data. We'll do an 80/20 split
set.seed(123)
Chol_split <- createDataPartition(Chol$LBDLDL, p=0.80, list=FALSE)
# split into train and test
Chol_train <- Chol_dummy[Chol_split,]
Chol_test <- Chol_dummy[-Chol_split,]
# split predictors from the target
Chol_train_X <- as.data.frame(subset(Chol_train, select=-c(LBDLDL)))
Chol_train_y <- Chol_train$LBDLDL
Chol_test_X <- as.data.frame(subset(Chol_test, select=-c(LBDLDL)))
Chol_test_y <- Chol_test$LBDLDL
# Creating imputed data sets
Chol_imp <- preProcess(Chol_train_X, method = c("center", "scale", "knnImpute"))
Chol_train_X_imp <- predict(Chol_imp, Chol_train_X)
Chol_test_X_imp <- predict(Chol_imp, Chol_test_X)
# Adding Resampling/Validation Set and Control
set.seed(123)
Chol_folds <- createFolds(y = Chol_train_X, k = 10, returnTrain = T)
Chol_control <- trainControl(method = "cv", index = Chol_folds)
Drop_col <- c('RIDRETH1', 'RIDRETH3', 'DMDBORN4', 'DMDEDUC2', 'DMDMARTZ', 'SIALANG', 'FIALANG', 'MIALANG', 'AIALANGA', 'LBDLDL')
Chol_num <- Chol_2 %>% select(-one_of(Drop_col))
Chol_dummy <- fastDummies::dummy_cols(Chol_num)
Chol_num <- Chol_dummy %>% select_if(~!is.factor(.))
Chol_num[] <- lapply(Chol_num, as.numeric)
Chol_num_tr_X <- as.data.frame(Chol_num[Chol_split, ])
Chol_num_test_X <- as.data.frame(Chol_num[-Chol_split, ])
#Preprocess
Chol_imp <- preProcess(Chol_num_tr_X, method = c("center", "scale", "knnImpute"))
Chol_num_tr_X <- predict(Chol_imp, Chol_num_tr_X)
Chol_num_test_X <- predict(Chol_imp, Chol_num_test_X)
# Adding Resampling/Validation Set and Control
set.seed(123)
Chol_folds_num <- createFolds(y = Chol_num_tr_X, k = 10, returnTrain = T)
Chol_control_num <- trainControl(method = "cv", index = Chol_folds_num)
Chol_ols_tune <- train(x = Chol_train_X_imp, y = Chol_train_y, method = "lm", trControl = Chol_control)
plot(Chol_ols_tune$finalModel)
# VIF shows aliased coefficients, need to get rid of those by removing high cor predictors
test <- cor(Chol_train_X_imp)
# Also have an issue with DMDEDUC2_DK all being zero so get rid of high var predictors
Chol_tr_x_imp_vr <- Chol_train_X_imp[, -nearZeroVar(Chol_train_X_imp)]
Chol_tr_X_imp_fin <- Chol_tr_x_imp_vr[, -findCorrelation(cor(Chol_tr_x_imp_vr), cutoff = 0.9)]
Chol_ols_tune2 <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "lm", trControl = Chol_control)
plot(Chol_ols_tune2$finalModel)
summary(Chol_ols_tune2$finalModel)
Chol_bct <- preProcess(Chol_tr_X_imp_fin, method = "BoxCox")
Chol_tr_boxcox <- predict(Chol_bct, Chol_tr_X_imp_fin)
Chol_ols_tune3 <- train(x = Chol_tr_boxcox, y = Chol_train_y, method = "lm", trControl = Chol_control)
plot(Chol_ols_tune3$finalModel)
summary(Chol_ols_tune3$finalModel)
Chol_ols_tune_num <- train(x = Chol_num_tr_X, y = Chol_train_y, method = "lm", trControl = Chol_control_num)
plot(Chol_ols_tune_num$finalModel)
summary(Chol_ols_tune_num$finalModel)
Chol_sig_tr <- Chol_tr_X_imp_fin %>% select(RIDAGEYR, BMXHT, BMXBMI, BMXLEG, BMXARMC,  DMDBORN4_Oth, DMDEDUC2_NA, MIALANG_NA, AIALANGA_NA)
Chol_ols <- train(x = Chol_sig_tr, y = Chol_train_y, method = "lm", trControl = Chol_control)
plot(Chol_ols$finalModel)
summary(Chol_ols$finalModel)
#Predict on test data
Chol_ols_res <- predict(Chol_ols, Chol_test_X)
set.seed(123)
Chol_pcr <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "pcr", tuneGrid = expand.grid(ncomp=1:32), trControl = Chol_control)
Chol_pcr
set.seed(123)
Chol_pcr_box <- train(x = Chol_tr_boxcox, y = Chol_train_y, method = "pcr", tuneGrid = expand.grid(ncomp=1:32), trControl = Chol_control)
Chol_pcr_box
set.seed(123)
Chol_pcr_num <- train(x = Chol_num_tr_X, y = Chol_train_y, method = "pcr", tuneGrid = expand.grid(ncomp=1:8), trControl = Chol_control_num)
Chol_pcr_num
pcr_resamp <- Chol_pcr$results
pcr_resamp$Model <- "PCR"
box_pcr_resamp <- Chol_pcr_box$results
box_pcr_resamp$Model <- "BPCR"
num_pcr_resamp <- Chol_pcr_num$results
num_pcr_resamp$Model <- "PCR"
set.seed(123)
Chol_pls <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "pls", tuneGrid = expand.grid(ncomp = 1:32), trControl = Chol_control)
Chol_pls
set.seed(123)
Chol_pls_box <- train(x = Chol_tr_boxcox, y = Chol_train_y, method = "pls", tuneGrid = expand.grid(ncomp = 1:32), trControl = Chol_control)
Chol_pls_box
set.seed(123)
Chol_pls_num <- train(x = Chol_num_tr_X, y = Chol_train_y, method = "pls", tuneGrid = expand.grid(ncomp = 1:8), trControl = Chol_control_num)
Chol_pls_num
pls_resamp <- Chol_pls$results
pls_resamp$Model <- "PLS"
pls_box_resamp <- Chol_pls_box$results
pls_box_resamp$Model <- "BPLS"
pls_num_resamp <- Chol_pls_num$results
pls_num_resamp$Model <- "PLS"
plot_data <- rbind(pcr_resamp, box_pcr_resamp, pls_resamp, pls_box_resamp)
xyplot(RMSE ~ ncomp, data = plot_data, xlab = "# of Components", ylab = "RMSE (Cross-validation)", auto.key = list(columns = 4), groups = Model, type = c("o", "g"))
plot2_data <- rbind(num_pcr_resamp, pls_num_resamp)
xyplot(RMSE ~ ncomp, data = plot2_data, xlab = "# of Components", ylab = "RMSE (Cross-validation)", auto.key = list(columns = 2), groups = Model, type = c("o", "g"))
set.seed(123)
Chol_ridge <- train(x = Chol_tr_X_imp_fin, y= Chol_train_y, method = "ridge", tuneGrid = expand.grid(lambda = seq(0, .1, length = 15)), trControl = Chol_control)
Chol_ridge
set.seed(123)
Chol_ridge_box <- train(x = Chol_tr_boxcox, y= Chol_train_y, method = "ridge", tuneGrid = expand.grid(lambda = seq(0, .5, length = 15)), trControl = Chol_control)
Chol_ridge_box
set.seed(123)
Chol_ridge_num <- train(x = Chol_num_tr_X, y= Chol_train_y, method = "ridge", tuneGrid = expand.grid(lambda = seq(0, .5, length = 15)), trControl = Chol_control_num)
Chol_ridge_num
print(update(plot(Chol_ridge), xlab = "Penalty"))
print(update(plot(Chol_ridge_box), xlab = "Penalty"))
print(update(plot(Chol_ridge_num), xlab = "Penalty"))
enet_grid <- expand.grid(lambda = c(0, 0.01, 0.1), fraction = seq(0.05, 1, length = 20))
Chol_enet <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "enet", tuneGrid = enet_grid, trControl = Chol_control)
Chol_enet
Chol_enet_box <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "enet", tuneGrid = enet_grid, trControl = Chol_control)
Chol_enet_box
Chol_enet_num <- train(x = Chol_num_tr_X, y = Chol_train_y, method = "enet", tuneGrid = enet_grid, trControl = Chol_control_num)
plot(Chol_enet)
plot(Chol_enet_box)
plot(Chol_enet_num)
Res_OLS <- predict(Chol_ols, Chol_test_X_imp)
Res_OLS_num <- predict(Chol_ols_tune_num, Chol_num_test_X)
Res_PLS <- predict(Chol_pls, Chol_test_X_imp)
Res_PLS_num <- predict(Chol_pls_num, Chol_num_test_X)
Res_PCR <- predict(Chol_pcr, Chol_test_X_imp)
Res_PCR_num <- predict(Chol_pcr_num, Chol_num_test_X)
Res_Ridge <- predict(Chol_ridge, Chol_test_X_imp)
Res_Ridge_num <- predict(Chol_ridge_num, Chol_num_test_X)
Res_Enet <- predict(Chol_enet, Chol_test_X_imp)
Res_Enet_num <- predict(Chol_enet_num, Chol_num_test_X)
Linear_res <- cbind.data.frame(Observed = Chol_test_y, OLS = Res_OLS, OLS_num = Res_OLS_num, PLS = Res_PLS, PLS_num = Res_PLS_num, PCR = Res_PCR, PCR_num = Res_PCR_num ,Ridge = Res_Ridge, Ridge_num = Res_Ridge_num, ENet = Res_Enet, ENet_num = Res_Enet_num)
find_rmse <- function(x){
caret::RMSE(x, Linear_res[,"Observed"])
}
RMSE_results <- apply(X = Linear_res[,2:11], FUN = find_rmse, MARGIN = 2)
RMSE_results <- data.frame(RMSE_results)
RMSE_results$Model <- rownames(RMSE_results)
ggplot(RMSE_results, aes(x=reorder(Model, -RMSE_results), y=RMSE_results)) + geom_segment(aes(x=reorder(Model, -RMSE_results), xend = reorder(Model, -RMSE_results), y=30, yend=RMSE_results), color = "cadetblue") + geom_point(color = "darkblue", size = 10) + coord_flip() + ylab("RMSE") + geom_text(aes(label = round(RMSE_results, 2)), color = "white", size = 2.5)
# initial SVM model with radial basis and processed Chol_tr_X_imp_fin and Chol_train_y
set.seed(123)
svmR0 <- train(x=Chol_tr_X_imp_fin, y = Chol_train_y,
method = "svmRadial",
preProcess = c("center", "scale"),
tuneLength = 14,
trControl = Chol_control)
svmR0
# final model uses: sigma = 0.0207376 and C = 0.25
# RMSE: 35.40559, Rsquared: 0.019455264
plot(svmR0, scales = list(x = list(log = 2)), main="SVM-Radial Initial")
# svm radial model v2
# issue causing variables in X: MIALANG_Spanish, DMDEDUC2_<9, MIALANG_NA (zero var)
Chol_tr_X_impfin_drop <- c("MIALANG_Spanish", "DMDEDUC2_<9", "MIALANG_NA")
Chol_tr_X_impfin_sv <- subset(Chol_tr_X_imp_fin,
select = !(names(Chol_tr_X_imp_fin) %in% Chol_tr_X_impfin_drop))
#making test X have same columns available
Chol_te_X_sv <- subset(Chol_test_X_imp, select = c(names(Chol_tr_X_impfin_sv)))
dim(Chol_tr_X_impfin_sv)
dim(Chol_te_X_sv)
# sigma grid instead of using tuneLength = 14
sigmaEst <- kernlab::sigest(as.matrix(Chol_tr_X_impfin_sv[,1:29]))
Csearch <- 2^seq(-4,+4)
# sigma estimates using kernlab's sigest function
svmgrid <- expand.grid(sigma = sigmaEst, C = Csearch)
#model
set.seed(123)
svmR1 <- train(x=Chol_tr_X_impfin_sv, y = Chol_train_y,
method = "svmRadial",
preProcess = c("center", "scale"),
tuneGrid = svmgrid,
trControl = Chol_control)
svmR1
# final model uses: sigma = 0.03504524 and C = 0.25.
# RMSE: 35.30443, Rsquared: 0.020245225
plot(svmR1, scales = list(x = list(log = 2)), main="SVM-Radial v2")
svmR1$finalModel
# going to use the x training set from final svm-radial due to assuming there will be the same problem causing factors of nearZeroVar.
set.seed(123)
svmP <- train(x=Chol_tr_X_impfin_sv, y = Chol_train_y,
method = "svmPoly",
preProcess = c("center", "scale"),
tuneGrid = expand.grid(degree = 1:3,
scale = c(0.01, 0.005, 0.001, 0.0005),
C = Csearch),
trControl = Chol_control)
svmP
# final model uses: degree = 2, scale =  0.001,  offset =  1
# sigma = 0.02231109 and C = 2.
# RMSE: 35.44929, Rsquared: 0.011746076
plot(svmP, scales = list(x = list(log = 2),
between=list(x=.5, y=1)), main="SVM-Polynomial")
svmP$finalModel
# KNN Model needs to have NZV removed so again we are using the x=Chol_tr_X_impfin_sv to train and Chol_te_X_sv to test
set.seed(123)
knnTune <- train(x=Chol_tr_X_impfin_sv, y = Chol_train_y,
method = "knn",
preProcess = c("center", "scale"),
tuneGrid = data.frame(k=1:40),
trControl = Chol_control)
knnTune
# final model uses: k=40
# RMSE: 35.63394, Rsquared: 0.0006650456
plot(knnTune, main="KNN")
knnTune$finalModel
# MARS model doesn't need preprocessing, so first rendition will be with Chol_tr_X_imp_fin
set.seed(123)
mars1 <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y,
method = "earth",
tuneGrid = expand.grid(degree = 1:3, nprune = 2:38),
trControl = Chol_control)
mars1$finalModel
mars1$results
#used 1 of 32 predictors, 2 of 18 terms (nprune =2) degree=1
# RMSE: 35.02993, Rsquared:	0.05354368
plot(mars1, main="MARS Initial")
# MARS model using same X sets as SVM models:
set.seed(123)
mars2 <- train(x = Chol_tr_X_impfin_sv, y = Chol_train_y,
method = "earth",
tuneGrid = expand.grid(degree = 1:3, nprune = 2:38),
trControl = Chol_control)
mars2$finalModel
plot(mars2, main="MARS Secondary")
# No change between the two MARS models. Drops all but two factors for both.
#nprune=2, degree=1, RMSE: 34.80857, Rsquared: 0.05457991
set.seed(123)
nnetGrid <- expand.grid(decay = c(0, 0.01, .1), size = c(3, 7, 11, 13))
# NNET first rendition will be with Chol_tr_X_imp_fin
set.seed(100)
nnet1 <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y,
method = "nnet",
tuneGrid = nnetGrid,
trControl = Chol_control,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 13 * (ncol(Chol_tr_X_imp_fin) + 1) + 13 + 1,
maxit = 100)
nnet1
# size=3, decay=0.1
# RMSE: 40.97813, Rsquared: 0.0051328366
plot(nnet1, main="NNET first")
set.seed(123)
# NNET second rendition will be with Chol_tr_X_impfin_sv
set.seed(100)
nnet2 <- train(x = Chol_tr_X_impfin_sv, y = Chol_train_y,
method = "nnet",
tuneGrid = nnetGrid,
trControl = Chol_control,
preProc = c("center", "scale"),
linout = TRUE,
trace = FALSE,
MaxNWts = 13 * (ncol(Chol_tr_X_impfin_sv) + 1) + 13 + 1,
maxit = 100)
nnet2
# size=13, decay=0.1
# RMSE: 44.38061, Rsquared: 0.001743654
plot(nnet2, main="NNET second")
NonLpred <- data.frame(obs=Chol_test_y)
NonLpred$svmR <- predict(svmR1, Chol_te_X_sv)
NonLpred$svmP <- predict(svmP, Chol_te_X_sv)
NonLpred$KNN <- predict(knnTune, Chol_te_X_sv)
NonLpred$MARS2 <- predict(mars2, Chol_te_X_sv)
NonLpred$NNET2 <- predict(nnet2, Chol_te_X_sv)
plotpred <- data.frame(x=1:921, y1=NonLpred$obs, y2=NonLpred$svmR,
y3=NonLpred$svmP, y4=NonLpred$KNN,
y5=NonLpred$MARS2[,"y"], y6=NonLpred$NNET2)
plot(plotpred$x, plotpred$y1, type = "l", col = 1,
xlab = "prediction #", ylab = "prediction values",
main = "Nonlinear Model Predictions")
lines(plotpred$x, plotpred$y2, col = 2)
lines(plotpred$x, plotpred$y3, col = 3)
lines(plotpred$x, plotpred$y4, col = 4)
lines(plotpred$x, plotpred$y5, col = 5)
lines(plotpred$x, plotpred$y6, col = 6)
legend("bottomleft", cex=0.5, legend = c("Observed", "SVM-Radial",
"SVM-Polynomial", "KNN", "MARS", "NNET"),
col = 1:6, lwd = 2)
# RMSE = sqrt(sum((obs-pred)^2)/n), n=921
getRMSE <- function(x,y) {
sqrt(sum((x-y)^2)/length(x))
}
nonlin_rmse <- data.frame(c("svmRad", "svmPoly", "KNN",
"MARS", "NNet"))
nonlin_rmse$RMSE <- c(getRMSE(NonLpred$obs, NonLpred$svmR),
getRMSE(NonLpred$obs, NonLpred$svmP),
getRMSE(NonLpred$obs, NonLpred$KNN),
getRMSE(NonLpred$obs, NonLpred$MARS2),
getRMSE(NonLpred$obs, NonLpred$NNET2))
colnames(nonlin_rmse)[1]<-"Model Type"
nonlin_rmse[order(nonlin_rmse$RMSE),]
# best non linear model is svm radial with
# sigma = sigma = 0.03504524 and C = 0.25. (RMSE is 32.95)
expDropC <- c("RIDRETH1_Black", "RIDRETH1_Mex", "RIDRETH1_OHis",
"RIDRETH1_Oth", "RIDRETH1_White", "BMXBMI")
Xtrial_tr <- subset(Chol_train_X_imp, select = !(names(Chol_train_X_imp) %in% expDropC))
# looking for near zero var:
Xtri_nzv <- nearZeroVar(Xtrial_tr)
# "DMDBORN4_Ref", "DMDBORN4_DK", "DMDEDUC2_Ref", "DMDEDUC2_DK", "DMDMARTZ_Ref",
# "DMDMARTZ_DK", "AIALANGA_Asian"
Xtri_tr_nz <- subset(Xtrial_tr, select = -c(Xtri_nzv))
print(paste("Xtrial_tr ncol: ", ncol(Xtrial_tr), " NZV removed, new ncol: ", ncol(Xtri_tr_nz)))
# dropping SIALANG groups (sample person interview instrument lang)
expDropC <- c("SIALANG_English", "SIALANG_Spanish",
"MIALANG_English", "MIALANG_Spanish", "MIALANG_NA")
Xtri_tr_nz <- subset(Xtri_tr_nz, select = !(names(Xtri_tr_nz) %in% expDropC))
print(paste("Xtrial_tr ncol: ", ncol(Xtrial_tr), " NZV removed, new ncol: ", ncol(Xtri_tr_nz)))
#looking for high corr:
Xtritr_hiC <- findCorrelation(cor(Xtri_tr_nz), cutoff = 0.8)
Xtritr_hiC
# "AIALANGA_English", "DMDBORN4_USA", "BMXWT", "FIALANG_English",
# "BMXWAIST", "BMXHIP", "DMDEDUC2_NA", "RIAGENDR_Male"
Xtritr_hiC <- subset(Xtri_tr_nz, select = c("AIALANGA_English", "DMDBORN4_USA",
"BMXWT", "FIALANG_English", "BMXWAIST",
"BMXHIP", "DMDEDUC2_NA", "RIAGENDR_Male"))
invisible(cor(Xtritr_hiC)) # invisible used to reduce extensive output
corrplot(cor(Xtritr_hiC), order = "hclust", type="lower")
#dropping "BMXHIP", "BMXWAIST", DMDEDUC2_<9 (recurring issues in model attempts)
drophiC <- c("BMXHIP", "BMXWAIST", "DMDEDUC2_<9")
X_trial_train <- subset(Xtri_tr_nz, select = !(names(Xtri_tr_nz) %in% drophiC))
corrplot(cor(X_trial_train), order = "hclust", type="lower", tl.cex = 0.7)
keepsies <- colnames(X_trial_train)
X_trial_test <- subset(Chol_test_X_imp, select = c(keepsies))
#svm radial with X_trial_train and log adjusted y
log_Y_train <- log10(Chol_train_y)
hist(log_Y_train)
log_Y_test <- log10(Chol_test_y)
#model
set.seed(123)
svmR_trial <- train(x=X_trial_train, y = log_Y_train,
method = "svmRadial",
preProcess = c("center", "scale"),
tuneLength = 14,
trControl = Chol_control)
svmR_trial
# issues in: DMDEDUC2_<9,
# final model uses: sigma = 0.02019005 and C = 0.25.
# RMSE: 0.1531765, Rsquared: 0.021637157
# while these are the lowest values, the graph is identical to the non-log adjusted SVM radial model with just different RMSE values.
plot(svmR_trial, scales = list(x = list(log = 2)), main="SVM-Radial with X_trial_train and Log Y")
svmR_trial$finalModel
trialPred <- data.frame(obs=log_Y_test)
trialPred$svmRad <- predict(svmR_trial, X_trial_test)
set.seed(123)
# let's see how it splits the training data
Chol_rpart <- rpart(Chol_train_y ~., data = Chol_tr_X_imp_fin, cp = 0.003)
summary(Chol_rpart)
rpart.plot(Chol_rpart)
# tune and predict
Chol_rpart_tune <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "rpart", cp = 0.003)
Chol_rpart_pred <- predict(Chol_rpart_tune, Chol_test_X_imp)
postResample(pred = Chol_rpart_pred, obs = Chol_test_y)
# Rsquared value of 0.066 isn't too great. RMSE of 33.0. Let's compare to other trees
set.seed(123)
Chol_rpart2_tune <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "rpart2", cp = 0.003)
Chol_rpart2_pred <- predict(Chol_rpart2_tune, Chol_test_X_imp)
postResample(pred = Chol_rpart2_pred, obs = Chol_test_y)
# minor improvement? Rsquared value of 0.071 and RMSE of 32.9
set.seed(123)
Chol_bagtree <- train(x=Chol_tr_X_imp_fin, y = Chol_train_y, method = "treebag", nbagg = 70, trControl = Chol_control)
Chol_bagtree
# Rsquared value of 0.033. RMSE is 35.3. Still not fantastic
set.seed(123)
rfmtryValues <- seq(1,10,1)
Chol_rf <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "rf", ntree = 300, tuneGrid = data.frame(mtry=rfmtryValues), trControl=Chol_control) # 300 trees provides the highest Rsquared value when checking with test data
Chol_rf
plot(Chol_rf)
Chol_rf_pred <- predict(Chol_rf, Chol_test_X_imp)
postResample(pred = Chol_rf_pred, obs = Chol_test_y)
# Rsquared value of 0.077. RMSE of 33.1
# some control parameters
gbmGrid <- expand.grid(interaction.depth = c(1,3,5,7,9), n.trees=300, shrinkage = c(0.01, 0.1), n.minobsinnode=5)
set.seed(123)
Chol_gbm <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "gbm", tuneGrid = gbmGrid, verbose = FALSE, trControl = Chol_control)
Chol_gbm
Chol_gbm_pred <- predict(Chol_gbm, Chol_test_X_imp)
postResample(pred = Chol_gbm_pred, obs = Chol_test_y)
# RMSE 32.9 and Rsquared 0.081
# decision tree with linear regression at terminal nodes to predict continuous variables
set.seed(123)
Chol_M5 <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "M5", trControl = Chol_control, control = Weka_control(M=10))
plot(Chol_M5)
Chol_M5_pred <- predict(Chol_M5, Chol_test_X_imp)
postResample(pred = Chol_M5_pred, obs = Chol_test_y)
# Rsquared value of 0.076 and RMSE of 33.6
# decision tree with linear regression at terminal nodes to predict continuous variables
set.seed(123)
Chol_M5rules <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "M5Rules", trControl = Chol_control, control = Weka_control(M=10))
plot(Chol_M5rules)
Chol_M5rules_pred <- predict(Chol_M5rules, Chol_test_X_imp)
postResample(pred = Chol_M5rules_pred, obs = Chol_test_y)
# Slight improvement, Rsquared value of 0.087 and RMSE of 32.8
set.seed(123)
Chol_cube <- train(x = Chol_tr_X_imp_fin, y = Chol_train_y, method = "cubist", trControl = Chol_control)
plot(Chol_cube)
Chol_cube_pred <- predict(Chol_cube, Chol_test_X_imp)
postResample(pred = Chol_cube_pred, obs = Chol_test_y)
# "Best performing" so far, but ever so slightly over the other models. Rsquared of 0.092 and RMSE of 32.6
NonLpred <- NonLpred %>% select(!obs)
Trees <- cbind.data.frame(Cubist = Chol_cube_pred, GBM = Chol_gbm_pred, M5 = Chol_M5_pred, M5Rules = Chol_M5rules_pred, RF = Chol_rf_pred, CART = Chol_rpart2_pred)
Results <- cbind(Linear_res, NonLpred, Trees)
find_rmse <- function(x){
caret::RMSE(x, Results[,"Observed"])
}
RMSE_results <- apply(X = Results[,2:22], FUN = find_rmse, MARGIN = 2)
RMSE_results <- data.frame(RMSE_results)
RMSE_results$Model <- rownames(RMSE_results)
RMSE_results$Model_Type <- "Linear"
RMSE_results$Model_Type[11:15] <- "Non-Linear"
RMSE_results$Model_Type[16:21] <- "Tree"
ggplot(RMSE_results, aes(x=reorder(Model, -RMSE_results), y=RMSE_results)) + geom_segment(aes(x=reorder(Model, -RMSE_results), xend = reorder(Model, -RMSE_results), y=30, yend=RMSE_results, color = Model_Type)) + geom_point(aes(color=Model_Type), size = 9) + coord_flip() + ylab("RMSE") + geom_text(aes(label = round(RMSE_results, 2)), color = "black", size = 2.5) + labs(color = "Model Type") + xlab("Model")
cubist_plot <- Results %>% select(Observed, Cubist)
cubist_plot$x <- c(1:921)
cubist_plot <- cubist_plot %>% gather(key = "Data Source", value = "LDL", -x)
# cubist_plot$alpha <- ifelse(cubist_plot$Observed == "Observed", 0.8, 1)
ggplot(cubist_plot, aes(x = x, y = LDL)) + geom_line(aes(color = `Data Source`, alpha = `Data Source`)) + scale_color_manual(values = c("royalblue1", "royalblue4")) + scale_alpha_manual(values = c(1,.3)) + theme_classic()
ggplot(RMSE_results, aes(x=reorder(Model, -RMSE_results), y=RMSE_results)) + geom_segment(aes(x=reorder(Model, -RMSE_results), xend = reorder(Model, -RMSE_results), y=30, yend=RMSE_results, color = Model_Type)) + geom_point(aes(color=Model_Type), size = 9) + coord_flip() + ylab("RMSE") + geom_text(aes(label = round(RMSE_results, 2)), color = "black", size = 2.5) + labs(color = "Model Type") + xlab("Model")
ggplot(RMSE_results, aes(x=reorder(Model, -RMSE_results), y=RMSE_results)) + geom_segment(aes(x=reorder(Model, -RMSE_results), xend = reorder(Model, -RMSE_results), y=30, yend=RMSE_results, color = Model_Type)) + geom_point(aes(color=Model_Type), size = 9) + coord_flip() + ylab("RMSE") + geom_text(aes(label = round(RMSE_results, 2)), color = "black", size = 2.5, fontface = "bold") + labs(color = "Model Type") + xlab("Model")
ggplot(RMSE_results, aes(x=reorder(Model, -RMSE_results), y=RMSE_results)) + geom_segment(aes(x=reorder(Model, -RMSE_results), xend = reorder(Model, -RMSE_results), y=30, yend=RMSE_results, color = Model_Type)) + geom_point(aes(color=Model_Type), size = 9) + coord_flip() + ylab("RMSE") + geom_text(aes(label = round(RMSE_results, 2)), color = "black", size = 2.5, fontface = "bold") + labs(color = "Model Type") + xlab("Model")
ggplot(RMSE_results, aes(x=reorder(Model, -RMSE_results), y=RMSE_results)) + geom_segment(aes(x=reorder(Model, -RMSE_results), xend = reorder(Model, -RMSE_results), y=30, yend=RMSE_results, color = Model_Type)) + geom_point(aes(color=Model_Type), size = 9) + coord_flip() + ylab("RMSE") + geom_text(aes(label = round(RMSE_results, 2)), color = "black", size = 2.5, fontface = "bold") + labs(color = "Model Type") + xlab("Model")
